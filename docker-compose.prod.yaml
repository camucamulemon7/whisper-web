services:
  backend:
    # Production image from GitHub Container Registry
    image: ghcr.io/camucamulemon7/whisper:1.4.0
    ports:
      - "${API_PORT:-8000}:${API_PORT:-8000}"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_MODULE_LOADING=LAZY
      - LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu:$LD_LIBRARY_PATH
      - CUDNN_PATH=/usr/lib/x86_64-linux-gnu
    env_file:
      - .env
    volumes:
      - whisper-models:/app/models
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${API_PORT:-8000}/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - whisper-network
    restart: unless-stopped

  frontend:
    # Production image from GitHub Container Registry
    image: ghcr.io/camucamulemon7/whisper-frontend:1.4.0
    ports:
      - "5173:5173"
    env_file:
      - .env
    depends_on:
      - backend
    networks:
      - whisper-network
    restart: unless-stopped

networks:
  whisper-network:
    driver: bridge

volumes:
  whisper-models:
